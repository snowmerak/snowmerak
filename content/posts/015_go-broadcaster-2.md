---
title: "하나의 프로세스 안에서 데이터 브로드캐스팅 하기 - 스위치와 큐"
date: 2022-11-20T09:29:16+09:00
tags: ["go", "concurrency", "broadcast"]
draft: false
---

## 스위치

스위치는 각 리스트를 관리하는 구조체입니다. 데이터가 들어오면 하나의 리스트에 모든 데이터를 추가할 것이 아니기 때문에, 각 리스트에 적절히 분배하는 로직이 필요합니다. 

### map + sync.Mutex와 sync.Map, 그리고 trie

어떤 리스트로 전달해주기 위해서는 해당 리스트의 이름으로, 해당 리스트의 포인터를 저장해놓을 필요가 있습니다. 가장 빠른 접근법이 `map`과 `sync.Mutex`를 이용해서 쓰레드 세이프한 맵을 만들거나, `sync.Map`을 활용하는 방법입니다. 하지만 이 방법들은 공통적으로 락을 사용하게 되고, 리스트가 많아질수록 접근하기 위해 쓰이는 비용도 늘어날 것입니다. 당연하게도 이 스위치 설계 안에서는 [`go-datastructures`의 `trie` 패키지](https://github.com/Workiva/go-datastructures/tree/master/trie)에 속한 `ctrie`와 `dtrie`같은 락프리 트라이 구현체를 사용할 것입니다. 

락프리이기에 락을 사용하는 구조에 비해 값 싸고 효율적인 구조가 나올 수 있다고 생각합니다.

### 인터페이스

그럼 스위치는 다음과 같은 메서드를 가진 인터페이스를 가집니다.

```go
type Switch interface {
    Add(name string, ttl int64) bool
    Iterate(name string, func(*Node)) bool
    Append(name string, value []byte) bool
    Remove(name string) bool
}
```

`Add` 메서드는 사실상의 이전 포스트에 나온 `List`의 생성자가 될 것입니다.  
`Iterate` 메서드는 `List`의 `Iterate`를 래핑한 메서드입니다. 내부적으로 트라이에서 검색한 후, 해당 리스트의 `Iterate`를 호출하는 형태로 될 것입니다.  
`Append` 메서드는 `List`를 찾아 추가하는 작업을 래핑한 메서드입니다. 이전 포스트에서도 언급했다시피 논리적으로 순차적으로 처리될 수 있게 조치할 것입니다.  
`Remove` 메서드는 해당 이름의 리스트를 트라이에서 제거합니다.  
모든 `bool` 타입 반환값은 트라이에 해당 이름이 존재하는 지를 나타냅니다. 

## 다시 리스트

이전 포스트에서 언급했던 대로, 리스트는 논리적으로 한번에 하나의 write 작업이 있어야 합니다. 이 작업은 두가지 방법이 있다고 봅니다. 하나는 `sync.Mutex`, `atomic.Int64`를 통해 쓰기락을 구현하는 것입니다. 그리고 다른 하나는 `channel`과 `go-datastructures`의 `ringbuffer`를 이용해서 큐를 만든 후, 고루틴 하나가 받아가며 처리하는 것입니다. 저는 이 중 순서를 어느정도 보장할 수 있으면서, 비교적 더 단순한 형태를 가져갈 수 있는 후자를 선택합니다.

### 큐

각 리스트는 생성되면 하나의 `obersving`을 위한 고루틴이 돌아갑니다. 리스트가 내부적으로 가지고 있는 `입력 대기 큐`를 계속 보고 있으면서, 추가되는 값이 있을 때마다 값을 가져와서 `push` 처리를 합니다. 그러면 이론적으로 한번에 하나의 데이터만 리스트에 계속 추가할 수 있습니다.

### 머리를 자르자

마지막으로 TTL이 지난 노드를 제거하는 트리거가 어디에 들어가야할지 고민해야합니다.

1. 큐에서 값을 꺼내어 데이터를 추가할 때, 검사하여 처리하는 방법이 합리적일 수는 있지만 오랫동안 값이 추가되지 않는 다면 지속적으로 쓰레기가 남게 될 것입니다.
2. 특정 주기를 가지고, 해당 주기마다 TTL을 검사해서 처리하는 방법은 하드웨어 자원을 아낄 수 있지만, 클라이언트에게 필요 이상으로 많은 정보를 줄 수 있습니다.
3. 스핀락처럼 지속적으로 TTL을 검사하는 방법은 클라이언트에게 정확한 정보를 주겠지만, 하드웨어 자원을 낭비하게 될 것입니다.

그래서 저는 2번과 3번을 섞어서, 스핀락처럼 계속 TTL을 검사하지만 그 주기가 서서히 늘어나는 방법을 채택합니다. 처음엔 8 나노초부터 시작해서 2배씩 커지게 되고, 끝내는 2048 밀리초까지 대기할 것입니다. 그러면 늦어도 2초 간격의 정보를 클라이언트가 잘못 받을 수는 있지만, 매우 긴 시간동안 메시지가 없었을 것이기 때문에 많은 양의 데이터가 잘못 전송되는 걸 막을 수 있습니다. 또 너무 자주 검사를 시도해서 하드웨어 자원을 필요 이상으로 낭비하는 것도 어느 정도 방지할 수 있을 것입니다. 

## 끝